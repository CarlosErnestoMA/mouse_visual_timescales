{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 12:12:45,081 | INFO | notebook | <module> | project directory: /data.nst/lucas/projects/mouse_visual_timescales_predictability/paper_code/experiment_analysis\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------ #\n",
    "# @Author:        F. Paul Spitzner\n",
    "# @Email:         paul.spitzner@ds.mpg.de\n",
    "# @Created:       2023-08-04 11:59:06\n",
    "# @Last Modified: 2024-03-23 16:30:22\n",
    "# ------------------------------------------------------------------------------ #\n",
    "# Run on the cluster, using dask.\n",
    "# Analyses all units and saves a large dataframe with everything\n",
    "# that is needed.\n",
    "# This is the notebook that analysis spontaneous activity.\n",
    "# ------------------------------------------------------------------------------ #\n",
    "\n",
    "%load_ext ipy_dict_hierarchy\n",
    "%load_ext watermark\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)-s | %(name)-s | %(funcName)-s | %(message)s\",\n",
    "    level=logging.WARNING,\n",
    ")\n",
    "log = logging.getLogger(\"notebook\")\n",
    "log.setLevel(\"DEBUG\")\n",
    "\n",
    "import re\n",
    "import glob\n",
    "import h5py\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import dask\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# also needs to be added for each dask-worker\n",
    "extra_path = os.path.abspath('../')\n",
    "sys.path.append(extra_path)\n",
    "log.info(f\"project directory: {extra_path}\")\n",
    "\n",
    "from ana import utility as utl\n",
    "utl.log.setLevel(\"ERROR\")\n",
    "\n",
    "# specify the path as closely as possible, we search for the spike files recursively through all subdirs\n",
    "data_dir = \"/path/to/repo/experiment_analysis/dat/\"\n",
    "# analysis results can sit in the same directory\n",
    "output_dir = \"/path/to/repo/experiment_analysis/dat/\"\n",
    "output_name = \"all_units_spontaneous_for_merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching metadata from sessions: 100%|████████| 58/58 [01:03<00:00,  1.09s/it]\n",
      "Loading spikes for sessions:  86%|██████████▎ | 50/58 [02:42<00:21,  2.67s/it]"
     ]
    }
   ],
   "source": [
    "meta_df = utl.all_unit_metadata(data_dir, reload=False)\n",
    "meta_df = utl.load_spikes(meta_df, format=\"numpy\")\n",
    "meta_df = utl.default_filter(meta_df, trim=False)  # only updates the status column\n",
    "\n",
    "# limit to spontaneous\n",
    "meta_df = meta_df[meta_df[\"stimulus\"] == \"spontaneous\"]\n",
    "meta_df[\"spiketimes\"] = meta_df.apply(\n",
    "    lambda row: utl.prepare_spike_times(row[\"spiketimes\"], \"spontaneous_for_merged\"),\n",
    "    axis=1,\n",
    ")\n",
    "meta_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(meta_df))\n",
    "print(len(meta_df.query(\"invalid_spiketimes_check == 'SUCCESS'\")))\n",
    "print(len(meta_df.query(\"invalid_spiketimes_check != 'SUCCESS'\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hde_settings = {\n",
    "    \"number_of_bootstraps_R_tot\": 0,\n",
    "    \"number_of_bootstraps_R_max\": 250,\n",
    "    \"timescale_minimum_past_range\": 30 / 1000,\n",
    "    \"embedding_number_of_bins_set\": [5],\n",
    "    \"estimation_method\": \"shuffling\",\n",
    "    \"persistent_analysis\": False,\n",
    "    # fmt: off\n",
    "    \"embedding_past_range_set\": [\n",
    "        0.005, 0.00561, 0.00629, 0.00706, 0.00792, 0.00889, 0.00998, 0.01119, 0.01256,\n",
    "        0.01409, 0.01581, 0.01774, 0.01991, 0.02233, 0.02506, 0.02812, 0.03155, 0.0354,\n",
    "        0.03972, 0.04456, 0.05, 0.0561, 0.06295, 0.06441, 0.06591, 0.06745, 0.06902,\n",
    "        0.07063, 0.07227, 0.07396, 0.07568, 0.07744, 0.07924, 0.08109, 0.08298, 0.08491,\n",
    "        0.08689, 0.08891, 0.09099, 0.0931, 0.09527, 0.09749, 0.09976, 0.10209, 0.10446,\n",
    "        0.1069, 0.10939, 0.11194, 0.11454, 0.11721, 0.11994, 0.12274, 0.12559, 0.12852,\n",
    "        0.13151, 0.13458, 0.13771, 0.14092, 0.1442, 0.14756, 0.151, 0.15451, 0.15811,\n",
    "        0.1618, 0.16557, 0.16942, 0.17337, 0.17741, 0.18154, 0.18577, 0.19009, 0.19452,\n",
    "        0.19905, 0.20369, 0.20843, 0.21329, 0.21826, 0.22334, 0.25059, 0.28117, 0.31548,\n",
    "        0.35397, 0.39716, 0.44563, 0.5, 0.56101, 0.62946, 0.70627, 0.79245, 0.88914,\n",
    "        0.99763, 1.11936, 1.25594, 1.40919, 1.58114, 1.77407, 1.99054, 2.23342, 2.50594,\n",
    "        2.81171, 3.15479, 3.53973, 3.97164, 4.45625, 5.0,\n",
    "    ],\n",
    "    # fmt: on\n",
    "}\n",
    "\n",
    "mre_settings = {\n",
    "    \"bin_size\": 0.005,  # 5 ms\n",
    "    \"tmin\": 0.03,\n",
    "    \"tmax\": 10.0,\n",
    "}\n",
    "\n",
    "\n",
    "def mre_wrapper(data, settings):\n",
    "\n",
    "    logging.getLogger(\"mrestimator\").setLevel(\"ERROR\")\n",
    "    import mrestimator as mre\n",
    "    mre.disable_progressbar()\n",
    "\n",
    "    data = data.squeeze()\n",
    "    assert data.ndim == 1, \"data must be 1D, this is the simple one-unit wrapper\"\n",
    "\n",
    "    binned_spikes = utl.binned_spike_count(data, bin_size=settings[\"bin_size\"])\n",
    "\n",
    "    rk = mre.coefficients(\n",
    "        binned_spikes,\n",
    "        method=\"ts\",  # method does not matter for single unit\n",
    "        steps=(\n",
    "            int(settings[\"tmin\"] / settings[\"bin_size\"]),\n",
    "            int(settings[\"tmax\"] / settings[\"bin_size\"]),\n",
    "        ),\n",
    "        dt=settings[\"bin_size\"],\n",
    "        dtunit=\"s\",\n",
    "    )\n",
    "\n",
    "    fit_single = mre.fit(rk, fitfunc=mre.f_exponential_offset)\n",
    "    fit_double = mre.fit(rk, fitfunc=mre.f_two_timescales)\n",
    "\n",
    "    details_single = fit_single._asdict()\n",
    "    details_double = fit_double._asdict()\n",
    "\n",
    "    details_single[\"fitfunc\"] = details_single[\"fitfunc\"].__name__\n",
    "    details_double[\"fitfunc\"] = details_double[\"fitfunc\"].__name__\n",
    "\n",
    "    for key in settings.keys():\n",
    "        details_single[key] = settings[key]\n",
    "        details_double[key] = settings[key]\n",
    "\n",
    "    # steps might get too big and are easy to reconstruct\n",
    "    details_single.pop(\"steps\", None)\n",
    "    details_double.pop(\"steps\", None)\n",
    "\n",
    "    res = {\n",
    "        \"tau_single\": fit_single.tau,\n",
    "        \"tau_double\": fit_double.tau,\n",
    "        \"details_tau_single\": details_single,\n",
    "        \"details_tau_double\": details_double,\n",
    "    }\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def full_analysis(spikes):\n",
    "    \"\"\"\n",
    "    Take one set of spikes, run hdestimator and mrestimator.\n",
    "\n",
    "    # Parameters\n",
    "    spikes : 1d numpy array\n",
    "        flat list of spike times for a single unit. nans are removed.\n",
    "    \"\"\"\n",
    "\n",
    "    logging.getLogger(\"hdestimator\").setLevel(\"ERROR\")\n",
    "    import hdestimator as hde\n",
    "\n",
    "    # remove nan-padding\n",
    "    spikes = spikes.squeeze()\n",
    "    spikes = spikes[np.isfinite(spikes)]\n",
    "\n",
    "    try:\n",
    "        hde_res = hde.api.wrapper(spike_times=spikes, settings=hde_settings)\n",
    "        for key in [\"plot_AIS\", \"plot_settings\", \"plot_color\", \"ANALYSIS_DIR\"]:\n",
    "            hde_res[\"settings\"].pop(key, None)\n",
    "    except:\n",
    "        hde_res = dict()\n",
    "        hde_res[\"R_tot\"] = np.nan\n",
    "        hde_res[\"tau_R\"] = np.nan\n",
    "\n",
    "    try:\n",
    "        mre_res = mre_wrapper(spikes, mre_settings)\n",
    "    except:\n",
    "        mre_res = dict()\n",
    "        mre_res[\"tau_single\"] = np.nan\n",
    "        mre_res[\"tau_double\"] = np.nan\n",
    "        mre_res[\"details_tau_single\"] = dict()\n",
    "        mre_res[\"details_tau_double\"] = dict()\n",
    "\n",
    "    return hde_res, mre_res\n",
    "\n",
    "\n",
    "def dask_it(itertuple):\n",
    "    \"\"\"\n",
    "    Idea is to delegate this to a dask worker and get a dictionary back\n",
    "    that has everything we need. Every worker gets a one-row dataframe\n",
    "\n",
    "    Retruns a tuple of (index, hde_res, mre_res)\n",
    "    index is a pandas index so we can re-insert into the outer dataframe,\n",
    "    the others are dicts with results\n",
    "    \"\"\"\n",
    "\n",
    "    index, iterrow = itertuple\n",
    "\n",
    "    # dont forget to prepare the spikes beforehand\n",
    "    spikes = iterrow[\"spiketimes\"].squeeze()\n",
    "\n",
    "    hde_res, mre_res = full_analysis(spikes)\n",
    "\n",
    "    return index, hde_res, mre_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually test for a single unit\n",
    "for itertuple in meta_df.iterrows():\n",
    "    index, hde_res, mre_res = dask_it(itertuple)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mre_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hde_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import tempfile\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from dask_jobqueue import SGECluster\n",
    "from dask.distributed import Client, SSHCluster, LocalCluster, as_completed\n",
    "from contextlib import nullcontext, ExitStack\n",
    "\n",
    "# silence dask, configure this in ~/.config/dask/logging.yaml to be reliable\n",
    "# https://docs.dask.org/en/latest/how-to/debug.html#logs\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logging.getLogger(\"dask\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"distributed\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"distributed.worker\").setLevel(logging.WARNING)\n",
    "\n",
    "num_cores = 128\n",
    "df_in_progress = None # for debugging, keep the global variable available\n",
    "\n",
    "def main(dask_client, prepared_df):\n",
    "\n",
    "    global df_in_progress, index, hde_res, mre_res\n",
    "    df_in_progress = prepared_df.copy()\n",
    "    df_in_progress[\"R_tot\"] = np.nan\n",
    "    df_in_progress[\"tau_R\"] = np.nan\n",
    "    df_in_progress[\"tau_single\"] = np.nan\n",
    "    df_in_progress[\"tau_double\"] = np.nan\n",
    "    df_in_progress[\"tau_R_details\"] = None\n",
    "    df_in_progress[\"tau_single_details\"] = None\n",
    "    df_in_progress[\"tau_double_details\"] = None\n",
    "\n",
    "    # dispatch, reading in parallel may be faster\n",
    "    # create a list from the iterator, as dask map \"no longer supports iterators\" -.-\n",
    "    futures = dask_client.map(dask_it, list(prepared_df.iterrows()))\n",
    "\n",
    "    log.info(\"futures dispatched\")\n",
    "\n",
    "    # save once an hours\n",
    "    last_save = time.time()\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), leave=True):\n",
    "\n",
    "        index, hde_res, mre_res = future.result()\n",
    "\n",
    "        # we use the .at[] method instead of .loc[] to be able to set\n",
    "        # cells to dictionaries. (df.at can only access a single value at a time\n",
    "        # df.loc can select multiple rows and/or columns)\n",
    "        # https://stackoverflow.com/questions/13842088/set-value-for-particular-cell-in-pandas-dataframe-using-index\n",
    "        df_in_progress.at[index, \"R_tot\"] = hde_res[\"R_tot\"]\n",
    "        df_in_progress.at[index, \"tau_R\"] = hde_res[\"tau_R\"]\n",
    "        df_in_progress.at[index, \"tau_single\"] = mre_res[\"tau_single\"]\n",
    "        df_in_progress.at[index, \"tau_double\"] = mre_res[\"tau_double\"]\n",
    "\n",
    "        df_in_progress.at[index, \"tau_R_details\"] = hde_res\n",
    "        df_in_progress.at[index, \"tau_single_details\"] = mre_res[\"details_tau_single\"]\n",
    "        df_in_progress.at[index, \"tau_double_details\"] = mre_res[\"details_tau_double\"]\n",
    "\n",
    "        # save every hour or so\n",
    "        if time.time() - last_save > 3600:\n",
    "            time_str = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "            try:\n",
    "                df_in_progress.reset_index().to_feather(f\"{output_dir}/{output_name}_{time_str}.feather\")\n",
    "            except Exception as e:\n",
    "                log.error(e)\n",
    "            last_save = time.time()\n",
    "\n",
    "    try:\n",
    "        df_in_progress.reset_index().to_feather(f\"{output_dir}/{output_name}_final.feather\")\n",
    "    except Exception as e:\n",
    "        log.error(e)\n",
    "\n",
    "    return df_in_progress\n",
    "\n",
    "\n",
    "with ExitStack() as stack:\n",
    "    # init dask using a context manager to ensure proper cleanup of remote compute.\n",
    "    # adapt as needed, details depend on your setup\n",
    "    dask_cluster = stack.enter_context(\n",
    "        # rudabeh\n",
    "        SGECluster(\n",
    "            cores=32,\n",
    "            memory=\"192GB\",\n",
    "            processes=32,\n",
    "            job_extra_directives=[\"-pe mvapich2-sam 32\"],\n",
    "            log_directory=\"/scratch01.local/pspitzner/dask/logs\",\n",
    "            local_directory=\"/scratch01.local/pspitzner/dask/scratch\",\n",
    "            interface=\"ib0\",\n",
    "            walltime=\"24:00:00\",\n",
    "            worker_extra_args=[\n",
    "                f'--preload \\'import sys; sys.path.append(\"{extra_path}\");\\''\n",
    "            ],\n",
    "        )\n",
    "        # or use local compute:\n",
    "        # LocalCluster(local_directory=f\"{tempfile.gettempdir()}/dask/\")\n",
    "    )\n",
    "    dask_cluster.scale(cores=num_cores)\n",
    "    dask_client = stack.enter_context(Client(dask_cluster))\n",
    "\n",
    "    final_df = main(dask_client, meta_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_hdf(f\"{output_dir}/{output_name}.h5\", key=\"meta_df\")\n",
    "\n",
    "# to combine the frames from sponatneous and stimulated activity see the notebook\n",
    "# combine_dataframes.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix group permissions for collaborative directories\n",
    "os.system(f\"chmod -R g+rwx {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark -v --iversions --packages mrestimator,hdestimator,scipy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
