{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext ipy_dict_hierarchy\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import zarr\n",
    "import xarray as xr\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s | %(levelname)-8s | %(name)-12s | %(message)s\",\n",
    "    level=logging.WARNING,\n",
    ")\n",
    "log = logging.getLogger(\"notebook\")\n",
    "log.setLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 20:50:43,693 | INFO     | notebook     | found 2400 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'dict'>\n",
       "├── N .................................................................... int  10000\n",
       "├── N_to_record .......................................................... int  100\n",
       "├── adjacency_matrix ..................................................... str  scipy.spars...\n",
       "├── current_timestep ..................................................... int  240000\n",
       "├── dataformat_details ................................................... str           Sp...\n",
       "├── dt ................................................................. float  0.005\n",
       "├── duration_equil ....................................................... int  1200\n",
       "├── duration_record ...................................................... int  1200\n",
       "├── gamma .............................................................. float  -598.4561146858672\n",
       "├── input_alpha .......................................................... str  None\n",
       "├── input_tau .......................................................... float  0.03\n",
       "├── input_type ........................................................... str  OU\n",
       "├── k .................................................................... int  100\n",
       "├── m .................................................................. float  0.97379\n",
       "├── m_AR ............................................................... float  0.9739560385102146\n",
       "├── meta_conda_exe ....................................................... str  /data.nst/p...\n",
       "├── meta_git_branch ...................................................... str  branching_n...\n",
       "├── meta_git_commit ...................................................... str  52f8db9358a...\n",
       "├── meta_git_commit_message .............................................. str  cleaned up ...\n",
       "├── meta_git_url ......................................................... str  git@gitlab....\n",
       "├── meta_hostname ........................................................ str  rostam070.l...\n",
       "├── meta_job_id .......................................................... str  7185254\n",
       "├── meta_note ............................................................ str  pauls run, ...\n",
       "├── meta_python_exe ...................................................... str  /data.nst/p...\n",
       "├── meta_sge_cwd_path .................................................... str  /data.nst/p...\n",
       "├── meta_sge_o_workdir ................................................... str  /data.nst/p...\n",
       "├── meta_sge_task_id ..................................................... str  374\n",
       "├── meta_username ........................................................ str  pspitzner\n",
       "├── num_timesteps ........................................................ int  480000\n",
       "├── num_timesteps_equil .................................................. int  240000\n",
       "├── num_timesteps_record ................................................. int  240000\n",
       "├── path_for_neuron_spiketimes ........................................... str  /data.nst/l...\n",
       "├── rate ............................................................... float  3.5\n",
       "├── rep .................................................................. int  13\n",
       "├── sigma .............................................................. float  0.01\n",
       "├── spike_cache_nids ..................................................... str  list with l...\n",
       "├── spike_cache_times .................................................... str  list with l...\n",
       "├── spikes_as_list ....................................................... str  zarr.core.A...\n",
       "├── spikes_by_neuron ..................................................... str  zarr.core.A...\n",
       "├── spikes_per_chunk ..................................................... int  1000000\n",
       "├── spikes_per_neuron .................................................... str  numpy.ndarr...\n",
       "├── state ................................................................ str  numpy.ndarr...\n",
       "├── target_activity .................................................... float  0.0175\n",
       "├── tau_gamma .......................................................... float  60.0\n",
       "├── ts_activity .......................................................... str  numpy.ndarr...\n",
       "├── ts_gamma ............................................................. str  numpy.ndarr...\n",
       "└── ts_rate .............................................................. str  numpy.ndarr..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_directory = \"/data.nst/lucas/history_dependence/signatures_of_temporal_processing_paper_code/data/bn_code_cleaned/\"\n",
    "output_filepath = \"/data.nst/share/projects/information_timescales/branching_network/res_dset_bn_code_cleaned_N=10_000_to_merge.zarr\"\n",
    "\n",
    "# I added a faster spike format \"spikes_by_neuron\", so we now have two groups\n",
    "# in the storage (and reading routine) to choose from.\n",
    "spike_group = \"/spikes_by_neuron\" # or spikes_as_list\n",
    "\n",
    "# dask settings, see last cell.\n",
    "num_cores = 256\n",
    "dump_progress = False # keep overwriting output_file as we make progress\n",
    "\n",
    "# we recorded 100 neurons per realization\n",
    "# A single analysis (one neuron, one core) takes ~ 420 sec\n",
    "N_to_analyze = 20\n",
    "\n",
    "# filenames are meaningless, we filter by metadata\n",
    "files = glob.glob(input_directory + f\"*.zarr\") \n",
    "log.info(f\"found {len(files)} files\")\n",
    "\n",
    "def get_dims_of_file(fname, dims_to_get='all'):\n",
    "    file = zarr.open(fname + spike_group, mode=\"r\")\n",
    "    dims = dict()\n",
    "\n",
    "    if dims_to_get == 'all':\n",
    "        dims_to_get = list(file.attrs.keys())\n",
    "\n",
    "    for dim in dims_to_get:\n",
    "        if dim in file.attrs:\n",
    "            dims[dim] = file.attrs[dim]\n",
    "\n",
    "    return dims\n",
    "\n",
    "get_dims_of_file(files[0], dims_to_get='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 20:51:30,357 | INFO     | notebook     | 800 files remaing after filtering\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates:\n",
      "  * nid         (nid) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19\n",
      "  * N           (N) int64 10000\n",
      "  * k           (k) int64 10\n",
      "  * m           (m) float64 0.8 0.8538 0.8849 0.905 ... 0.9725 0.9738 0.975\n",
      "  * rep         (rep) int64 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19\n",
      "  * input_type  (input_type) <U8 'OU' 'constant'\n"
     ]
    }
   ],
   "source": [
    "# create an xarray that maps coordinates to filenames\n",
    "\n",
    "# those will become the axis of the xarray\n",
    "dims_to_get = [\"N\", \"k\", \"m\", \"rep\", \"input_type\"]\n",
    "occurrences = {d: [] for d in dims_to_get}\n",
    "\n",
    "# we want to create an xarray that maps coordinates to filenames\n",
    "# I did not manage to this in a single run, so we have to `get_dims_of_file` twice:\n",
    "# once to find what coordinate combinations to expect, and then to create the map\n",
    "filtered_files = []\n",
    "for fname in files:\n",
    "    dims = get_dims_of_file(fname, dims_to_get)\n",
    "    \n",
    "    # if need be, we could filter here.\n",
    "    if dims[\"N\"] != 10_000 or dims[\"k\"] != 10:\n",
    "        continue\n",
    "    # else:\n",
    "    #     dims.pop(\"N\")\n",
    "\n",
    "    filtered_files.append(fname)\n",
    "\n",
    "    for k, v in dims.items():\n",
    "        occurrences[k].append(v)\n",
    "\n",
    "log.info(f\"{len(filtered_files)} files remaing after filtering\")\n",
    "\n",
    "# since we have repetitions / degeneracies, we need to filter unique occurences\n",
    "occurrences = {k: np.sort(np.unique(v)) for k, v in occurrences.items()}\n",
    "\n",
    "# create an empty array to fill\n",
    "cs_to_fname = xr.DataArray(\n",
    "    data=None,\n",
    "    dims = list(occurrences.keys()),\n",
    "    coords = list(occurrences.values()),\n",
    ")\n",
    "\n",
    "# create the map\n",
    "for fname in filtered_files: \n",
    "    dims = get_dims_of_file(fname, dims_to_get)\n",
    "    cs_to_fname.loc[dims] = fname\n",
    "\n",
    "# add the neuron dimension, (multiple neurons share the same file)\n",
    "cs_to_fname = cs_to_fname.expand_dims({\"nid\": np.arange(N_to_analyze)})\n",
    "\n",
    "# for the results we will fill an xarray dataset\n",
    "res_dset = xr.Dataset(\n",
    "    coords = cs_to_fname.coords,\n",
    ")\n",
    "\n",
    "# now we can get filenames easily:\n",
    "# cs_to_fname.sel(rep=0, k=10, m=0.95, nid=2).values[()]\n",
    "print(cs_to_fname.coords)\n",
    "\n",
    "# Note to future paul: maybe its better to just crawl the files in each worker\n",
    "# and pick the right filename. this seems really fast with zarr.\n",
    "# currently we send the (large?) cs_to_fname frame to every worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True, parallel=True, fastmath=False, cache=True)\n",
    "def binned_spike_count(spiketimes, bin_size, length=None):\n",
    "    \"\"\"\n",
    "    Similar to a population_rate, but we get a number of spike counts, per neuron,\n",
    "    as needed for e.g. cross-correlations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    spiketimes :\n",
    "        np array with first dim neurons, second dim spiketimes. nan-padded\n",
    "    bin_size :\n",
    "        float, in units of spiketimes\n",
    "    length :\n",
    "        duration of output trains, in units of spiketimes. Default: None,\n",
    "        uses last spiketime\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    counts : 2d array\n",
    "        time series of the counted number of spikes per bin,\n",
    "        one row for each neuron, in steps of bin_size\n",
    "    \"\"\"\n",
    "\n",
    "    num_n = spiketimes.shape[0]\n",
    "\n",
    "    if length is not None:\n",
    "        num_bins = int(np.ceil(length / bin_size))\n",
    "    else:\n",
    "        t_min = 0.0\n",
    "        t_max = np.nanmax(spiketimes)\n",
    "        num_bins = int(np.ceil((t_max - t_min) / bin_size)) + 1\n",
    "\n",
    "    counts = np.zeros(shape=(num_n, num_bins))\n",
    "\n",
    "    for n_id in range(0, num_n):\n",
    "        train = spiketimes[n_id]\n",
    "        for t in train:\n",
    "            if not np.isfinite(t):\n",
    "                break\n",
    "            t_idx = int(t / bin_size)\n",
    "            counts[n_id, t_idx] += 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mrestimation(\n",
    "    single_neuron_spiketimes,\n",
    "    bin_size=5,\n",
    "    dtunit=\"ms\",\n",
    "    tmin=0,\n",
    "    tmax=10000,\n",
    "    plot_autocorrelation = False,\n",
    "):\n",
    "    import mrestimator as mre\n",
    "    import logging\n",
    "    mre.log.disabled = True\n",
    "    logging.getLogger(\"mrestimator\").disabled=True\n",
    "\n",
    "    # binned spiking, input is in seconds\n",
    "    binned_spt = mre.input_handler(\n",
    "        binned_spike_count(single_neuron_spiketimes[np.newaxis,:], bin_size/1000),\n",
    "    )\n",
    "\n",
    "    # we use a double exponential to get two timescales\n",
    "    def f_two_timescales(k, tau1, A1, tau2, A2):\n",
    "        return np.abs(A1) * np.exp(-k / tau1) + np.abs(A2) * np.exp(-k / tau2)\n",
    "\n",
    "    # inital conditions to try when fitting: tau1, A1, tau2, A2\n",
    "    fitpars_two_timescales = np.array(\n",
    "        [\n",
    "            (0.1, 0.01, 10, 0.01),\n",
    "            (0.1, 0.1, 10, 0.01),\n",
    "            (0.5, 0.01, 10, 0.001),\n",
    "            (0.5, 0.1, 10, 0.01),\n",
    "            (0.1, 0.01, 10, 0),\n",
    "            (0.1, 0.1, 10, 0),\n",
    "            (0.5, 0.01, 10, 0),\n",
    "            (0.5, 0.1, 10, 0),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # now we work in ms\n",
    "    rk = mre.coefficients(\n",
    "        binned_spt,\n",
    "        method=\"ts\",\n",
    "        steps=(int(tmin / bin_size), int(tmax / bin_size)),\n",
    "        dt=bin_size,\n",
    "        dtunit=dtunit,\n",
    "    )\n",
    "\n",
    "    fit_two_timescales = mre.fit(\n",
    "        rk, fitpars=fitpars_two_timescales, fitfunc=f_two_timescales\n",
    "    )\n",
    "    fit_single_timescale = mre.fit(rk, fitfunc=mre.f_exponential_offset)\n",
    "    if plot_autocorrelation:\n",
    "        fig,ax = plt.subplots()\n",
    "        mre_out = mre.OutputHandler(rk, ax)\n",
    "        mre_out.add_coefficients(rk, color=\"g\", lw=0.5, alpha=0.6)\n",
    "        mre_out.add_fit(fit_two_timescales, color=\"g\")#, lw=0.5)\n",
    "        mre_out.add_fit(fit_single_timescale, color=\"b\")#, lw=0.5)\n",
    "        ax.set_ylabel('C(T)')\n",
    "        ax.set_xlabel('time lag $T$ (ms)')\n",
    "        plt.show()\n",
    "\n",
    "    tau_1_two_timescales = fit_two_timescales.popt[0]\n",
    "    A_1_two_timescales = np.abs(fit_two_timescales.popt[1])\n",
    "    tau_2_two_timescales = fit_two_timescales.popt[2]\n",
    "    A_2_two_timescales = np.abs(fit_two_timescales.popt[3])\n",
    "    # Choose the timescale with higher coefficient A\n",
    "\n",
    "    res = dict()\n",
    "    res[\"tau_C_twots_Amax\"] = (tau_1_two_timescales, tau_2_two_timescales)[\n",
    "        np.argmax((A_1_two_timescales, A_2_two_timescales))\n",
    "    ]\n",
    "    res[\"tau_C_twots_Amin\"] = (tau_1_two_timescales, tau_2_two_timescales)[\n",
    "        np.argmin((A_1_two_timescales, A_2_two_timescales))\n",
    "    ]\n",
    "    res[\"tau_C_single\"] = fit_single_timescale.tau\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/data.nst/pspitzner/information_timescales/branching_network/ana\")\n",
    "\n",
    "# these kind of imports often confuse dask\n",
    "# from ana.hdestimator_wrapper import hde\n",
    "from hdestimator_wrapper import hde\n",
    "\n",
    "def analyse_neuron(coords, spike_group=\"/spikes_by_neuron\", plot_autocorrelation = False):\n",
    "    \"\"\"\n",
    "    The combined analysis for a single neuron, including hde and mre.\n",
    "\n",
    "    Returns a dict with everything we found, and the (passed) coordinates so we can\n",
    "    asynchroniously store into the xarray.\n",
    "    \"\"\"\n",
    "\n",
    "    # in dask we do not want to infer the coordinates\n",
    "    fname = cs_to_fname.sel(coords).values[()]\n",
    "    dset = zarr.open(fname + spike_group, mode=\"r\")\n",
    "    # get single-neuron spiketimes, depending on storage format.\n",
    "    if spike_group == \"/spikes_as_list\":\n",
    "        nids = dset[0, :]\n",
    "        idx = np.where(nids == coords[\"nid\"])[0]\n",
    "        spike_times = dset[1, idx] * dset.attrs[\"dt\"]\n",
    "    elif spike_group == \"/spikes_by_neuron\":\n",
    "        spike_times = dset[coords[\"nid\"], :] * dset.attrs[\"dt\"]\n",
    "        spike_times = spike_times[spike_times >= 0]\n",
    "\n",
    "    # res will be a dict with a bunch of keys\n",
    "    res_c = mrestimation(spike_times, plot_autocorrelation = plot_autocorrelation)\n",
    "\n",
    "    # cli args (including the location of the settings file) are hard-coded in the wrapper\n",
    "    # currently, settings are adjusted in ana/hdestimator_settings.yaml\n",
    "    res_r = hde(spike_times)\n",
    "\n",
    "    # some extra observables that are easy to get\n",
    "    res_m_AR = {\"m_AR\": dset.attrs[\"m_AR\"]}\n",
    "    res_rate = {\"rate\": len(spike_times)/1200.}\n",
    "    \n",
    "    # merge the dicts, and return the coordinates so we can write to the dataset\n",
    "    return {**res_r, **res_c, **res_m_AR, **res_rate}, coords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'125.0 KiB'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import humanize\n",
    "f\"{humanize.naturalsize(cs_to_fname.nbytes, binary=True)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-09 12:50:45,071 | INFO     | notebook     | 16000 parameter combinations to analyse\n"
     ]
    }
   ],
   "source": [
    "# get all possible combinations of coordinates from res_dset\n",
    "combinations = list(itertools.product(*[res_dset.coords[d] for d in res_dset.dims]))\n",
    "combinations = [dict(zip(res_dset.dims, c)) for c in combinations]\n",
    "combinations = [{k: v.values[()] for k, v in c.items()} for c in combinations]\n",
    "\n",
    "# A single analysis (one neuron, one core) takes ~ 420 sec\n",
    "log.info(f\"{len(combinations)} parameter combinations to analyse\")\n",
    "\n",
    "# to test the analysis, we could now run it on a single neuron\n",
    "# this_cs = dict(rep=0, k=100, m=0.995, nid=0)\n",
    "# this_cs = combinations[0]\n",
    "# this_res, this_cs = analyse_neuron(this_cs, spike_group, plot_autocorrelation=True)\n",
    "# this_res.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from dask_jobqueue import SGECluster\n",
    "from dask.distributed import Client, SSHCluster, LocalCluster, as_completed\n",
    "from contextlib import nullcontext, ExitStack\n",
    "\n",
    "# silence dask, configure this in ~/.config/dask/logging.yaml to be reliable\n",
    "# https://docs.dask.org/en/latest/how-to/debug.html#logs\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "logging.getLogger(\"dask\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"distributed\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"distributed.worker\").setLevel(logging.WARNING)\n",
    "\n",
    "\n",
    "def main(dask_client):\n",
    "    \n",
    "    # global res_dset\n",
    "    \n",
    "    # dispatch, reading in parallel may be faster\n",
    "    futures = dask_client.map(analyse_neuron, combinations)\n",
    "    \n",
    "    log.info(\"futures dispatched\")\n",
    "    \n",
    "    idx = 0\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        idx += 1\n",
    "        # a dict of results observable -> scalar\n",
    "        # and a dict fo coordinates\n",
    "        this_res, this_cs = future.result()\n",
    "\n",
    "        # add datavariables to dset if they do not exist yet\n",
    "        for k in this_res.keys():\n",
    "            if k not in res_dset.data_vars:\n",
    "                res_dset[k] = xr.DataArray(\n",
    "                    np.nan,\n",
    "                    coords=res_dset.coords,\n",
    "                )\n",
    "\n",
    "        # write all results to the dataset\n",
    "        for k, v in this_res.items():\n",
    "            try:\n",
    "                res_dset[k].loc[this_cs] = v\n",
    "            except ValueError:\n",
    "                # numeric types only\n",
    "                res_dset[k].loc[this_cs] = np.nan\n",
    "\n",
    "        # analysis might be slow, lets save the progress\n",
    "        if dump_progress and idx % num_cores == 0:\n",
    "            try:\n",
    "                res_dset.to_zarr(output_filepath, mode=\"w\")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    try:\n",
    "        res_dset.to_zarr(output_filepath, mode=\"w\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    return res_dset\n",
    "\n",
    "with ExitStack() as stack:\n",
    "    # init dask using a context manager to ensure proper cleanup\n",
    "    # when using remote compute\n",
    "    dask_cluster = stack.enter_context(\n",
    "        # rudabeh\n",
    "        SGECluster(\n",
    "            cores=32,\n",
    "            memory=\"192GB\",\n",
    "            processes=32,\n",
    "            job_extra_directives=[\"-pe mvapich2-sam 32\"],\n",
    "            log_directory=\"/scratch01.local/pspitzner/dask/logs\",\n",
    "            local_directory=\"/scratch01.local/pspitzner/dask/scratch\",\n",
    "            # log_directory=\"/scratch02.local/johannes/dask/logs\",\n",
    "            # local_directory=\"/scratch02.local/johannes/dask/scratch\",\n",
    "            # log_directory=\"/scratch03.local/lucas/dask/logs\",\n",
    "            # local_directory=\"/scratch03.local/lucas/dask/scratch\",\n",
    "            interface=\"ib0\",\n",
    "            walltime='24:00:00',\n",
    "            worker_extra_args=[\n",
    "                '--preload \\'import sys; sys.path.append(\"/data.nst/pspitzner/information_timescales/branching_network/ana/\"); sys.path.append(\"/data.nst/pspitzner/information_timescales/branching_network/\");\\''\n",
    "            ],\n",
    "        )\n",
    "        # local cluster\n",
    "        # LocalCluster(local_directory=f\"{tempfile.gettempdir()}/dask/\")\n",
    "    )\n",
    "    dask_cluster.scale(cores=num_cores)\n",
    "    dask_client = stack.enter_context(Client(dask_cluster))\n",
    "\n",
    "    xr_dset = main(dask_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_dset_loaded = xr.open_zarr(output_filepath)\n",
    "np.where(np.isnan(res_dset['tau_C_single']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this cell shows how we can add dimensions that we might forgot about in a previous ana\n",
    "# res_dset_loaded =xr.open_zarr(output_filepath.replace(\".zarr\", \"_backup.zarr\"))\n",
    "\n",
    "# # load dims from an old simulation file\n",
    "# files = glob.glob(input_directory + f\"*230308*.zarr\")\n",
    "# print(f\"{len(files)} files matched the old pattern\")\n",
    "\n",
    "# new_dims = get_dims_of_file(files[0], dims_to_get=[\"k\", \"N\"])\n",
    "\n",
    "# # expand_dims: \"If provided as a dict, then the keys are the new dimensions and the\n",
    "# # values are either integers (giving the length of the new dimensions) or\n",
    "# # sequence/ndarray (giving the coordinates of the new dimensions)\n",
    "# new_dims = {k: [v] for k, v in new_dims.items()}\n",
    "# res_dset_loaded = res_dset_loaded.expand_dims(new_dims)\n",
    "# res_dset_loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save back to disk\n",
    "# res_dset_loaded.to_zarr(output_filepath.replace(\".zarr\", \"_added_dims.zarr\"), mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x7f62c512e570>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this cell is for merging two existing datasets\n",
    "\n",
    "# ds1 = xr.open_zarr(\"/data.nst/share/projects/information_timescales/branching_network/res_dset_bn_code_cleaned.zarr\")\n",
    "# ds2 = xr.open_zarr(\"/data.nst/share/projects/information_timescales/branching_network/res_dset_bn_code_cleaned_N=1000_to_merge.zarr\")\n",
    "# ds1 = ds1.merge(ds2)\n",
    "# ds1.to_zarr(\"/data.nst/share/projects/information_timescales/branching_network/res_dset_bn_code_cleaned_merged.zarr\", mode=\"w\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "its_bn",
   "language": "python",
   "name": "its_bn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "460cebcf42a29d2a3767433152578fc1498b8edb5244132769d3d43ec5bdad9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
